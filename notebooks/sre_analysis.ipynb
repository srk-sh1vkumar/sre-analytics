{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRE Performance Analysis Notebook\n",
    "\n",
    "This notebook provides interactive analysis of SRE metrics and performance data collected from AppDynamics and other monitoring sources.\n",
    "\n",
    "## Contents\n",
    "1. Data Loading and Exploration\n",
    "2. SLO/SLA Analysis\n",
    "3. Trend Analysis\n",
    "4. Incident Correlation\n",
    "5. Predictive Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Import our SRE modules\n",
    "from collectors.oauth_appdynamics_collector import OAuthAppDynamicsCollector\n",
    "from reports.enhanced_sre_report_system import EnhancedSREReportSystem\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest SRE report data\n",
    "def load_latest_report_data():\n",
    "    \"\"\"Load the most recent SRE report data\"\"\"\n",
    "    reports_dir = '../reports/generated'\n",
    "    \n",
    "    # Find the latest JSON report\n",
    "    json_files = [f for f in os.listdir(reports_dir) if f.endswith('.json') and 'sre_data' in f]\n",
    "    if not json_files:\n",
    "        print(\"No report data found. Generate a report first.\")\n",
    "        return None\n",
    "    \n",
    "    latest_file = max(json_files)\n",
    "    file_path = os.path.join(reports_dir, latest_file)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded data from: {latest_file}\")\n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "sre_data = load_latest_report_data()\n",
    "if sre_data:\n",
    "    print(f\"Report generated at: {sre_data['report_metadata']['generated_at']}\")\n",
    "    print(f\"Application: {sre_data['report_metadata']['application_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SLO metrics to DataFrame for analysis\n",
    "if sre_data and 'slo_metrics' in sre_data:\n",
    "    metrics_df = pd.DataFrame(sre_data['slo_metrics'])\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    metrics_df['timestamp'] = pd.to_datetime(metrics_df['timestamp'])\n",
    "    \n",
    "    print(f\"Loaded {len(metrics_df)} metrics\")\n",
    "    print(\"\\nMetrics Summary:\")\n",
    "    print(metrics_df.groupby(['service_name', 'metric_name'])['status'].value_counts())\n",
    "    \n",
    "    # Display first few rows\n",
    "    metrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SLO/SLA Compliance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLO Compliance Dashboard\n",
    "if 'metrics_df' in locals():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Compliance by Service\n",
    "    compliance_by_service = metrics_df.groupby('service_name')['status'].apply(\n",
    "        lambda x: (x == 'compliant').sum() / len(x) * 100\n",
    "    )\n",
    "    \n",
    "    axes[0, 0].bar(compliance_by_service.index, compliance_by_service.values)\n",
    "    axes[0, 0].set_title('SLO Compliance by Service (%)')\n",
    "    axes[0, 0].set_ylabel('Compliance %')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Error Budget Consumption\n",
    "    error_budget = metrics_df.groupby('service_name')['error_budget_consumed'].mean()\n",
    "    colors = ['red' if x > 75 else 'orange' if x > 50 else 'green' for x in error_budget.values]\n",
    "    \n",
    "    axes[0, 1].bar(error_budget.index, error_budget.values, color=colors)\n",
    "    axes[0, 1].set_title('Error Budget Consumption (%)')\n",
    "    axes[0, 1].set_ylabel('Budget Consumed %')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Current vs Target Values\n",
    "    availability_metrics = metrics_df[metrics_df['metric_name'] == 'availability']\n",
    "    x_pos = np.arange(len(availability_metrics))\n",
    "    \n",
    "    axes[1, 0].bar(x_pos - 0.2, availability_metrics['current_value'], 0.4, \n",
    "                   label='Current', alpha=0.7)\n",
    "    axes[1, 0].bar(x_pos + 0.2, availability_metrics['slo_target'], 0.4, \n",
    "                   label='SLO Target', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Services')\n",
    "    axes[1, 0].set_ylabel('Availability %')\n",
    "    axes[1, 0].set_title('Availability: Current vs SLO Target')\n",
    "    axes[1, 0].set_xticks(x_pos)\n",
    "    axes[1, 0].set_xticklabels(availability_metrics['service_name'], rotation=45)\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Status Distribution\n",
    "    status_counts = metrics_df['status'].value_counts()\n",
    "    colors_pie = {'compliant': 'green', 'at_risk': 'orange', 'breached': 'red'}\n",
    "    pie_colors = [colors_pie.get(status, 'gray') for status in status_counts.index]\n",
    "    \n",
    "    axes[1, 1].pie(status_counts.values, labels=status_counts.index, \n",
    "                   colors=pie_colors, autopct='%1.1f%%')\n",
    "    axes[1, 1].set_title('Overall SLO Status Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No metrics data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trend analysis using historical data\n",
    "def analyze_trends():\n",
    "    \"\"\"Analyze performance trends over time\"\"\"\n",
    "    \n",
    "    # Generate sample trend data (in production, this would come from time series DB)\n",
    "    services = ['web-service', 'api-service', 'auth-service', 'database-service']\n",
    "    metrics = ['availability', 'latency_p95', 'error_rate']\n",
    "    \n",
    "    # Create 30 days of sample data\n",
    "    dates = pd.date_range(start='2024-08-01', periods=30, freq='D')\n",
    "    \n",
    "    trend_data = []\n",
    "    \n",
    "    for service in services:\n",
    "        for metric in metrics:\n",
    "            if metric == 'availability':\n",
    "                # Availability trend (99.5% - 99.99%)\n",
    "                base_value = 99.9\n",
    "                values = base_value + np.random.normal(0, 0.05, len(dates))\n",
    "                values = np.clip(values, 99.5, 99.99)\n",
    "            elif metric == 'latency_p95':\n",
    "                # Latency trend (100ms - 300ms)\n",
    "                base_value = 200\n",
    "                values = base_value + np.random.normal(0, 20, len(dates))\n",
    "                values = np.clip(values, 100, 300)\n",
    "            else:  # error_rate\n",
    "                # Error rate trend (0% - 1%)\n",
    "                base_value = 0.1\n",
    "                values = base_value + np.abs(np.random.normal(0, 0.05, len(dates)))\n",
    "                values = np.clip(values, 0, 1)\n",
    "            \n",
    "            for date, value in zip(dates, values):\n",
    "                trend_data.append({\n",
    "                    'date': date,\n",
    "                    'service': service,\n",
    "                    'metric': metric,\n",
    "                    'value': value\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(trend_data)\n",
    "\n",
    "# Generate and plot trends\n",
    "trend_df = analyze_trends()\n",
    "\n",
    "# Create interactive plots with Plotly\n",
    "for metric in ['availability', 'latency_p95', 'error_rate']:\n",
    "    metric_data = trend_df[trend_df['metric'] == metric]\n",
    "    \n",
    "    fig = px.line(metric_data, x='date', y='value', color='service',\n",
    "                  title=f'{metric.replace(\"_\", \" \").title()} Trend (30 Days)',\n",
    "                  labels={'value': metric.replace('_', ' ').title()})\n",
    "    \n",
    "    fig.update_layout(height=400)\n",
    "    fig.show()\n",
    "\n",
    "print(\"üìà Trend analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Incident Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze incident data if available\n",
    "if sre_data and 'incident' in sre_data and sre_data['incident']:\n",
    "    incident = sre_data['incident']\n",
    "    \n",
    "    print(\"üö® Incident Analysis\")\n",
    "    print(f\"Incident ID: {incident['incident_id']}\")\n",
    "    print(f\"Severity: {incident['severity']}\")\n",
    "    print(f\"Duration: {incident['start_time']} to {incident['end_time']}\")\n",
    "    print(f\"Affected Services: {', '.join(incident['affected_services'])}\")\n",
    "    print(f\"\\nRoot Cause: {incident['root_cause']}\")\n",
    "    \n",
    "    # Create incident timeline visualization\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    start_time = pd.to_datetime(incident['start_time'])\n",
    "    end_time = pd.to_datetime(incident['end_time']) if incident['end_time'] else datetime.now()\n",
    "    \n",
    "    for i, service in enumerate(incident['affected_services']):\n",
    "        fig.add_shape(\n",
    "            type=\"rect\",\n",
    "            x0=start_time, x1=end_time,\n",
    "            y0=i, y1=i+0.8,\n",
    "            fillcolor=\"red\", opacity=0.6,\n",
    "            line=dict(color=\"red\", width=2)\n",
    "        )\n",
    "        \n",
    "        fig.add_annotation(\n",
    "            x=start_time + (end_time - start_time) / 2,\n",
    "            y=i + 0.4,\n",
    "            text=service,\n",
    "            showarrow=False,\n",
    "            font=dict(color=\"white\", size=10)\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Incident Timeline - Affected Services\",\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Services\",\n",
    "        yaxis=dict(tickmode='array', tickvals=list(range(len(incident['affected_services']))),\n",
    "                   ticktext=incident['affected_services']),\n",
    "        height=300\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Display LLM analysis\n",
    "    if incident.get('llm_analysis'):\n",
    "        print(\"\\nü§ñ AI-Powered Analysis:\")\n",
    "        print(incident['llm_analysis'][:500] + \"...\" if len(incident['llm_analysis']) > 500 else incident['llm_analysis'])\n",
    "else:\n",
    "    print(\"No incident data available in the current report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictive Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple predictive analysis using trend data\n",
    "def predict_slo_breach_risk(trend_df):\n",
    "    \"\"\"Predict the risk of SLO breaches based on trends\"\"\"\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    for service in trend_df['service'].unique():\n",
    "        service_data = trend_df[trend_df['service'] == service]\n",
    "        \n",
    "        risk_scores = {}\n",
    "        \n",
    "        for metric in ['availability', 'latency_p95', 'error_rate']:\n",
    "            metric_data = service_data[service_data['metric'] == metric]\n",
    "            \n",
    "            if len(metric_data) > 5:\n",
    "                # Calculate trend slope\n",
    "                x = np.arange(len(metric_data))\n",
    "                y = metric_data['value'].values\n",
    "                slope = np.polyfit(x, y, 1)[0]\n",
    "                \n",
    "                # Calculate volatility (standard deviation)\n",
    "                volatility = np.std(y)\n",
    "                \n",
    "                # Risk calculation (simplified)\n",
    "                if metric == 'availability':\n",
    "                    # For availability, negative slope is bad\n",
    "                    risk = max(0, -slope * 100) + volatility * 10\n",
    "                else:\n",
    "                    # For latency and error rate, positive slope is bad\n",
    "                    risk = max(0, slope * 10) + volatility * 5\n",
    "                \n",
    "                risk_scores[metric] = min(100, risk)  # Cap at 100\n",
    "        \n",
    "        # Overall risk is the maximum of individual metric risks\n",
    "        overall_risk = max(risk_scores.values()) if risk_scores else 0\n",
    "        predictions[service] = {\n",
    "            'overall_risk': overall_risk,\n",
    "            'metric_risks': risk_scores,\n",
    "            'risk_level': 'High' if overall_risk > 70 else 'Medium' if overall_risk > 40 else 'Low'\n",
    "        }\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Generate predictions\n",
    "predictions = predict_slo_breach_risk(trend_df)\n",
    "\n",
    "# Display predictions\n",
    "print(\"üîÆ SLO Breach Risk Predictions (Next 7 Days)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for service, pred in predictions.items():\n",
    "    print(f\"\\n{service}:\")\n",
    "    print(f\"  Overall Risk: {pred['overall_risk']:.1f}% ({pred['risk_level']})\")\n",
    "    for metric, risk in pred['metric_risks'].items():\n",
    "        print(f\"  {metric}: {risk:.1f}%\")\n",
    "\n",
    "# Create risk visualization\n",
    "services = list(predictions.keys())\n",
    "risks = [predictions[s]['overall_risk'] for s in services]\n",
    "colors = ['red' if r > 70 else 'orange' if r > 40 else 'green' for r in risks]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(x=services, y=risks, marker_color=colors)\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"SLO Breach Risk Prediction by Service\",\n",
    "    xaxis_title=\"Service\",\n",
    "    yaxis_title=\"Risk Score (%)\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate New Report\n",
    "\n",
    "Generate a fresh SRE report with the latest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new comprehensive report\n",
    "def generate_fresh_report():\n",
    "    \"\"\"Generate a new SRE report with current data\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Generating fresh SRE report...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize report system\n",
    "        system = EnhancedSREReportSystem(app_name='Interactive Analysis Demo')\n",
    "        \n",
    "        # Generate services list\n",
    "        services = ['web-frontend', 'api-gateway', 'user-service', 'order-service', 'payment-service']\n",
    "        \n",
    "        # Create incident scenario (2 hours ago)\n",
    "        incident_time = datetime.now() - timedelta(hours=2)\n",
    "        \n",
    "        # Generate comprehensive report\n",
    "        report_paths = system.generate_full_report_suite(\n",
    "            application_name='Interactive Analysis Demo',\n",
    "            services=services,\n",
    "            incident_time=incident_time,\n",
    "            incident_duration=1.0\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Fresh report generated!\")\n",
    "        for report_type, path in report_paths.items():\n",
    "            if path:\n",
    "                print(f\"  üìä {report_type.replace('_', ' ').title()}: {path}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Report generation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Uncomment to generate a fresh report\n",
    "# generate_fresh_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive analysis framework for SRE performance data. Key capabilities include:\n",
    "\n",
    "- **Real-time SLO monitoring** with compliance tracking\n",
    "- **Trend analysis** for performance prediction\n",
    "- **Incident correlation** with root cause analysis\n",
    "- **Predictive analytics** for proactive SRE management\n",
    "- **Interactive visualizations** for better insights\n",
    "\n",
    "### Next Steps:\n",
    "1. Connect to live AppDynamics data for real-time analysis\n",
    "2. Implement more sophisticated ML models for prediction\n",
    "3. Add automated alerting based on risk predictions\n",
    "4. Integrate with incident management systems\n",
    "5. Create custom dashboards for different stakeholders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}